import pickle

import tensorflow as tf
from transformers import TFRobertaModel, RobertaTokenizer, BertTokenizer, TFBertModel
import streamlit as st
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
import joblib
import os

# é…ç½®é¡µé¢
st.set_page_config(
    page_title="æƒ…æ„Ÿåˆ†æèŠå¤©æœºå™¨äºº",
    page_icon="ğŸ’¬",
    layout="wide"
)

# æ¨¡å‹è·¯å¾„é…ç½®
MODEL_PATHS = {
    "RoBERTa": r"Roberta\best_roberta_model.h5",
    "BERT": r"BERT\best_bert_model.h5",
    "CNN": r"CNN\final_cnn_model.h5",
    "RNN": r"CNN\final_cnn_model.h5",
    "LSTM": r"LSTM\best_lstm_model.h5",
    "SVM": r"SVM\SVM_model.pkl",
    "éšæœºæ£®æ—": r"SVM\SVM_model.pkl",
    "æœ´ç´ è´å¶æ–¯": r"æœ´ç´ è´å¶æ–¯\beiysesi_model.pkl",
}


# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
def check_model_files():
    missing_models = []
    for name, path in MODEL_PATHS.items():
        if not os.path.exists(path):
            missing_models.append(f"{name}æ¨¡å‹ ({path})")
    return missing_models


# ç¼“å­˜æ¨¡å‹åŠ è½½
@st.cache_resource
def load_model(model_name):
    if model_name in ["RoBERTa", "BERT"]:
        # åŠ è½½Transformeræ¨¡å‹
        if model_name == "RoBERTa":
            tokenizer = RobertaTokenizer.from_pretrained(r'RoBerta\roberta-base')
            base_model = TFRobertaModel.from_pretrained(r'RoBerta\roberta-base')
        else:  # BERT
            tokenizer = BertTokenizer.from_pretrained(r'BERT\bert-base-uncased')
            base_model = TFBertModel.from_pretrained(r'BERT\bert-base-uncased')

        max_length = 128

        # æ„å»ºæ¨¡å‹
        input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')
        attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')

        outputs = base_model(input_ids, attention_mask=attention_mask)
        pooled = outputs[1]

        x = tf.keras.layers.Dropout(0.3)(pooled)
        output = tf.keras.layers.Dense(1, activation='sigmoid')(x)

        model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)
        model.compile(optimizer=tf.keras.optimizers.Adam(2e-5),
                      loss='binary_crossentropy',
                      metrics=['accuracy'])

        model.load_weights(MODEL_PATHS[model_name])

        return {
            'model': model,
            'tokenizer': tokenizer,
            'max_length': max_length,
            'type': 'transformer'
        }

    elif model_name in ["CNN", "LSTM", "RNN"]:
        # åŠ è½½æ·±åº¦å­¦ä¹ æ¨¡å‹
        max_length = 100

        if model_name == "CNN":
            model_path = r'CNN\final_cnn_model.h5'
            tokenizer_path = r'CNN\tokenizer.pkl'
        elif model_name == "LSTM":
            model_path = r'LSTM\best_lstm_model.h5'
            tokenizer_path = r'LSTM\lstmtokenizer.pkl'
        else:
            model_path=r"CNN\final_cnn_model.h5"
            tokenizer_path=r"CNN\tokenizer.pkl"
        model = tf.keras.models.load_model(model_path)
        with open(tokenizer_path, 'rb') as f:
            tokenizer = pickle.load(f)

        return {
            'model': model,
            'tokenizer': tokenizer,
            'max_length': max_length,
            'type': 'deep_learning'
        }

    else:  # SVM, éšæœºæ£®æ—, æœ´ç´ è´å¶æ–¯
        # åŠ è½½æœºå™¨å­¦ä¹ æ¨¡å‹
        model = joblib.load(MODEL_PATHS[model_name])
        vectorizer = joblib.load("vectorizer.pkl")

        return {
            'model': model,
            'vectorizer': vectorizer,
            'type': 'machine_learning'
        }


# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'messages' not in st.session_state:
    st.session_state.messages = []
if 'current_model' not in st.session_state:
    st.session_state.current_model = "RoBERTa"
if 'model_loaded' not in st.session_state:
    missing_models = check_model_files()
    if missing_models:
        st.warning(f"ä»¥ä¸‹æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {', '.join(missing_models)}")
        st.warning("è¿™äº›æ¨¡å‹å°†æ— æ³•ä½¿ç”¨ï¼Œè¯·ç¡®ä¿æ¨¡å‹æ–‡ä»¶è·¯å¾„æ­£ç¡®ã€‚")
    try:
        st.session_state.model_info = load_model(st.session_state.current_model)
        st.session_state.model_loaded = True
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        st.stop()


# å®šä¹‰é¢„æµ‹å‡½æ•°
def predict_sentiment(text):
    model_info = st.session_state.model_info

    if model_info['type'] == 'transformer':
        # Transformeræ¨¡å‹ (RoBERTa, BERT)
        encoded = model_info['tokenizer'](
            text,
            max_length=model_info['max_length'],
            padding='max_length',
            truncation=True,
            return_tensors='tf'
        )

        pred = model_info['model'].predict([encoded['input_ids'], encoded['attention_mask']])
        score = float(pred[0][0])

    elif model_info['type'] == 'deep_learning':
        # æ·±åº¦å­¦ä¹ æ¨¡å‹ (CNN, RNN, LSTM)
        from tensorflow.keras.preprocessing.sequence import pad_sequences

        # æ–‡æœ¬å‘é‡åŒ–
        sequence = model_info['tokenizer'].texts_to_sequences([text])
        padded_sequence = pad_sequences(sequence, maxlen=model_info['max_length'])

        pred = model_info['model'].predict(padded_sequence)
        score = float(pred[0][0])


    else:  # æœºå™¨å­¦ä¹ æ¨¡å‹

        # ç›´æ¥ä½¿ç”¨åŸå§‹æ–‡æœ¬è¿›è¡Œé¢„æµ‹

        if hasattr(model_info['model'], 'predict_proba'):

            pred_proba = model_info['model'].predict_proba([text])

            score = float(pred_proba[0][1])  # å‡è®¾ç´¢å¼•1æ˜¯ç§¯æç±»

        else:

            pred = model_info['model'].predict([text])

            score = float(pred[0])

    sentiment = "ç§¯æ" if score > 0.5 else "æ¶ˆæ"
    return sentiment, score


# èŠå¤©ç•Œé¢
st.title("æƒ…æ„Ÿåˆ†æèŠå¤©æœºå™¨äºº")
st.markdown(f"ğŸ’¬ ä½¿ç”¨ **{st.session_state.current_model}** æ¨¡å‹åˆ†ææ–‡æœ¬æƒ…æ„Ÿå€¾å‘ï¼ˆç§¯æ/æ¶ˆæï¼‰")

# æ˜¾ç¤ºèŠå¤©å†å²
for msg in st.session_state.messages:
    with st.chat_message(msg['role']):
        st.markdown(msg['content'])
        if msg['role'] == 'assistant':
            # æ ¹æ®æƒ…æ„Ÿç»“æœæ˜¾ç¤ºä¸åŒé¢œè‰²
            if msg['sentiment'] == 'ç§¯æ':
                st.success(f"æƒ…æ„Ÿ: {msg['sentiment']} (æ¦‚ç‡: {msg['score']:.4f})")
                st.progress(msg['score'])
            else:
                st.error(f"æƒ…æ„Ÿ: {msg['sentiment']} (æ¦‚ç‡: {1 - msg['score']:.4f})")
                st.progress(1 - msg['score'])
            st.caption(f"ä½¿ç”¨ {msg['model']} æ¨¡å‹åˆ†æ")

# ç”¨æˆ·è¾“å…¥æ¡†
user_input = st.chat_input("è¯·è¾“å…¥è¦åˆ†æçš„æ–‡æœ¬...")

if user_input:
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    st.session_state.messages.append({
        'role': 'user',
        'content': user_input
    })

    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    with st.chat_message('user'):
        st.markdown(user_input)

    # ç”ŸæˆAIå›å¤
    with st.chat_message('assistant'):
        with st.spinner(f"æ­£åœ¨ä½¿ç”¨ {st.session_state.current_model} æ¨¡å‹åˆ†ææƒ…æ„Ÿ..."):
            sentiment, score = predict_sentiment(user_input)
            st.markdown(user_input)
            # æ ¹æ®æƒ…æ„Ÿç»“æœæ˜¾ç¤ºä¸åŒé¢œè‰²
            if sentiment == 'ç§¯æ':
                st.success(f"æƒ…æ„Ÿ: {sentiment} (æ¦‚ç‡: {score:.4f})")
                st.progress(score)
            else:
                st.error(f"æƒ…æ„Ÿ: {sentiment} (æ¦‚ç‡: {1 - score:.4f})")
                st.progress(1 - score)
            st.caption(f"ä½¿ç”¨ {st.session_state.current_model} æ¨¡å‹åˆ†æ")

    # æ·»åŠ AIå›å¤åˆ°å†å²
    st.session_state.messages.append({
        'role': 'assistant',
        'content': user_input,
        'sentiment': sentiment,
        'score': score,
        'model': st.session_state.current_model
    })

# ä¾§è¾¹æ  - æ¨¡å‹é€‰æ‹©å’Œæ¸…ç©ºèŠå¤©åŠŸèƒ½
with st.sidebar:
    st.header("æ¨¡å‹é€‰æ‹©")
    selected_model = st.selectbox(
        "é€‰æ‹©æƒ…æ„Ÿåˆ†ææ¨¡å‹",
        list(MODEL_PATHS.keys()),
        index=list(MODEL_PATHS.keys()).index(st.session_state.current_model)
    )

    if selected_model != st.session_state.current_model:
        with st.spinner(f"æ­£åœ¨åŠ è½½ {selected_model} æ¨¡å‹..."):
            try:
                st.session_state.model_info = load_model(selected_model)
                st.session_state.current_model = selected_model
                st.success(f"{selected_model} æ¨¡å‹åŠ è½½æˆåŠŸï¼")
                st.experimental_rerun()  # åˆ·æ–°é¡µé¢åº”ç”¨æ–°æ¨¡å‹
            except Exception as e:
                st.error(f"åŠ è½½ {selected_model} æ¨¡å‹å¤±è´¥: {str(e)}")

    st.header("åŠŸèƒ½é€‰é¡¹")
    if st.button("æ¸…ç©ºèŠå¤©è®°å½•"):
        st.session_state.messages = []
        st.experimental_rerun()

# é¡µè„š
st.markdown("""
---
*æ”¯æŒå¤šç§æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼ŒåŒ…æ‹¬æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ æ–¹æ³•*
""")